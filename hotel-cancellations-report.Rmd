---
title: "Classifying Hotel Cancellation"
author: "Connor O'Shea, James Lee, Kevin Kuo, Mani Singh"
date: "4/13/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

#Introduction of the Problem

  In 2020, COVID-19 has significantly impacted the day-to-day life of people everywhere. As the majority of the country has been placed in a shelter-in-place-esce way of life, the economic impact of this virus has began to show its true form. Millions of people have been left unemployed, small business have been shut down, and the market has struggled to return back to normalcy. During all this economic downturn, one of the hardest hit industries that have been hit by this is the hotel industry. STR, a hospitality research company, recently reported that about occupancy is down nearly 70% compared to last year[^1]:{https://www.usatoday.com/story/travel/hotels/2020/04/09/coronavirus-leaves-hotel-rooms-empty-str-data/5120441002/}. When everything returns to a place of normalcy, every hotel will be looking for ways to save as much profits as possible.
  The problem that we have attempted to solve this semester focuses on saving hotels money through an attempt at being able to figure out when a consumer is going to cancel a hotel reservation. While hotels receive a cancellation fee from a consumer opting out of a reservation, this is small fee that only represents approximately 1%  of total operating costs for a single year, meaning that high cancellations lead to inability for a company to function[^2]:{https://www.viggosmarthotel.com/hotel-booking-cancellations-challenge/}. As there has been a rise in the use of Online Travel Agencies (also known as OTAs), there has also been a rise of cancellations as the expereince has become easier, leading to more strain on the industry. We hope to be able to provide some relief through our ability to classify and the possible implications that are associated with successful classification. 

#Dataset
  We found our dataset from Kaggle.com[^3]:{https://www.kaggle.com/jessemostipak/hotel-booking-demand}. After further research into the dataset, we found that it originated from an experiment conducted by Nuno Antoin, Ana de Almeida and Luis Nunes who had published their work "Hotel Booking Demand Datasets" in the February 2019 issue of *Data in Brief*. The original purpose of this dataset was to determine the number of bookings a hotel would have on a certain day, which we believed made it perfect for classifying cancellations. The dataset consisted of data from two hotels located in Portugal. One was a resort located in Algarve, while the other was a city hotel located in Lisbon. It contained booking data for reservations from July 1, 2015 to August 31, 2018, containing 119390 observations (66% of which came from the city hotel). The dataset contained 33 variables, which included the Booking Channel from which the reservation was made, the lead time which represented the time before the reservation, and the country the individual was from. 

#Data Collection
  In addition to the dataset we had received from Kaggle, we wanted to find some supplemental data that we believed would have influence on whether an individual would cancel. For this we focused on four pieces of supplemental data:
*Temperature*
  Due to the fact that one of the major reasons for travel is related to vacation, we decided that data related to temperature may be important to record. To do so, we recorded the high, average, and low temperature for both Lisbon and Algarve by gathering data that was easily copied and pasted from a weather site from July 1, 2015 to August 31, 2018[^4]:{https://www.wunderground.com/history/monthly/pt/montenegro/LPFR/date/2017-4}. 
*Precipitation*
  As we had numerous people traveling, we felt that precipation could be a reason people cancel as the flight could be delayed/canceled and also if it is a rainy week, an individual may cancel their upcoming trip. To collect this data, we began by manually grabbing the first 400 days from a weather website[^5]:{https://www.worldweatheronline.com/lang/en-us/lisbon-weather-history/lisboa/pt.aspx}. Realizing this process was too tedious and taking hours to collect, we developed a code in Python to scrape the maximum points of rain we would from the API (1000 points a day). We have included the code below to serve as a point of reference. From this point we created a binary variable where if it rains there is a 1 for if it rained that day, 0 if it had not read that day. The reason we did that was due to issues we had in recording exact measurements of the total of rain from a singular day.
  
```{python}
from darksky.api import DarkSky
from darksky.types import languages, units, weather
from datetime import datetime as dt
import numpy as np


# key needed to access dark sky api
API_KEY = 'e7a30ac223ddd32b8afd34c868279d7f'

# dark sky api object
darksky = DarkSky(API_KEY)


# Function that returns csv containing the daily precipitation values
def get_data(year, latitude, longitude, start_month, end_month):
    # Storing the daily precipitaion values
    daily_precip = []
    # Accounting for leap years
    feb_days = 28
    if year % 4 == 0:
        feb_days = 29
        
    # Getting a subset of the months that we want
    wanted_keys = [i for i in range(start_month, end_month + 1)]
        
    months = {1: 31, 2: feb_days, 3: 31, 4: 30, 5: 31, 6: 30, 7: 31, 8: 31, 9: 30, 10: 31, 11: 30, 12: 31}
    
    # Creating a new dictionary with the subset
    months_subset = dict((k, months[k]) for k in wanted_keys if k in months)
    
    for i in months_subset:
        for j in range(1, months_subset[i] + 1):
            t = dt(year, i, j)
            forecast = darksky.get_time_machine_forecast(
                latitude, longitude,
                extend=False, # default `False`
                lang=languages.ENGLISH, # default `ENGLISH`
                values_units=units.AUTO, # default `auto`
                exclude=[weather.MINUTELY, weather.ALERTS], # default `[]`,
                timezone='UTC', # default None - will be set by DarkSky API automatically
                time=t
            )
            
        
            # Adding the precipitation to the list 
            daily_forecast = forecast.daily
            daily_data = daily_forecast.data
            print(daily_data)
            daily_precip.append(daily_data[0].precip_intensity_max)
            print(daily_precip)
        
    # Exporting results to a csv 
    daily_precip_np = np.array(daily_precip)
    file_name = str(start_month) + "-" + str(end_month) + "-" + str(year) + ".csv"
    np.savetxt(file_name, daily_precip_np, delimiter=",")
    
    return daily_precip


function_test = get_data(2015, 38.722252, -9.139337, 4, 12)
print(function_test)

test2 = get_data(2016, 38.722252, -9.139337, 4, 12)
test3 = get_data(2017, 38.722252, -9.139337, 4, 9)
test4 = get_data(2015, 38.638760, -9.037660, 1, 12)
test5 = get_data(2016, 38.638760, -9.037660, 1, 12)
test6 = get_data(2017, 38.638760, -9.037660, 1, 12)
```

*KM from Portugal*
  We believed that an individual who lived further away had a higher likelihood to cancel due to the fact that a small issue such as sickness or family could quickly deter plans. The first step in being able to determine if this would be true, was determining which countries we had in our dataset. Using the table function, we found the 178 different country codes used in our dataset. As the country code provided is 3 letters, we had to determine which code corresponded to what country. Using the World Integrated Trade Solution website [^6]:{https://wits.worldbank.org/wits/wits/witshelp/content/codes/country_codes.htm}, we went through and found each code individually and wrote in an Excel document what country corresponded to which code. After we took this step, we next looked up the distance that each country was from Portugal in kilometers [^7]:{https://www.google.com/maps and https://www.rome2rio.com/}. Once again, each country look up had to be done at an individual level as there was no dataset that contained this information for us that we could find. In the case of Portugal, we set the distance as 0 due to the fact that this was the country the hotel reservations were made for and we were not provided more granular information at the specific city level of travel. 
*European Union*
  We wanted to test to see if a person who booked a reservation came from the European Union, as we felt that this could lead to an person being able to freely travel in all other member states. To construct this variable, we created a new column and set it to be a binary variable where if a country was in the EU, they were given the value of 1 and if a country was not it was given a value of 0 [^8]:{https://europa.eu/european-union/about-eu/countries_en}. Overall, we have 28 EU countries, which included the United Kingdom as our data was from 2015 through 2018 and the United Kingdom did not leave the European Union until January 2020.
*Joining All The Data* 
  Once all the supplemental data was collected by our team, we attached this to our original data using the merge function, as demonstrated below: 
```{r}

```

#Analysis

```{r}

```
#Limitations

#Implications 
  The ability to predict the cancellation of hotel reservations is important as it presents a potential recipe to a problem of loss of revenue. If a hotel had the ability to determine the number of hotel cancellations will happen in a singular night, the hotel can potentially overbook with some certainity to decrease the number of empty room and maximize revenue. One industry that has done this already is the airport industry. According to the Bureau of Transportation Statistics, the number of trips where people denied board, whether for overbooking purposes or for other reasons was 0.09% in 2015[^8]:{https://www.ft.com/content/e4cb5744-1e9d-11e7-a454-ab04428977f9}. In the cases of overbooking, passengers are offered vouchers and potentially a complimentary flight. While this does demonstrate a small loss, 99.91% of the time, companies will remain significantly profiting from this strategy. Lastly, we would hope that by including other variables as we have in our models, we will be able to reduce this 0.09% overbooked rate as we know that there are possible backlashes with this, as demonstrated by the United Continental public relation disaster in April of 2017, where a customer was forcibly removed. We would also need to develop a backup plan for what would happen if there was overbooking. 
  One of our biggest reasons we choose this problem is that fact it has so many implications in numerous industries besides hospitality. By having the ability to overbook, more customers can be served. This is something that can have as little of an effect of a couple gets to have a dinner reservation instead of being on a waitlist or as big as a patient getting to see a doctor that would not have been able to for months. We hope that by demonstrating that this method can have success, that eventually this is something that will be accepted over time. 

#Conclusions 